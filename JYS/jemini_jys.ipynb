{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# E-commerce Classification: Baseline Selection & Tuning\n",
                "\n",
                "This notebook uses a **Two-Stage** approach:\n",
                "1. **Baseline Evaluation**: Train all candidate models with default parameters to identify the most promising architectures.\n",
                "2. **Hyperparameter Tuning**: Use Optuna to optimize ONLY the **Top 5** performing baseline models.\n",
                "3. **Ensemble**: Build Stacking and Voting ensembles from the tuned top models.\n",
                "\n",
                "## Candidate Models\n",
                "- Logistic Regression, Random Forest, Extra Trees, XGBoost, LightGBM, CatBoost, SVM, KNN"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import optuna\n",
                "import warnings\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\n",
                "\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, StackingClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from xgboost import XGBClassifier\n",
                "from lightgbm import LGBMClassifier\n",
                "from catboost import CatBoostClassifier\n",
                "\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading & Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_df = pd.read_csv('data/train_df.csv')\n",
                "test_df = pd.read_csv('data/test_df.csv')\n",
                "train_df.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def apply_feature_engineering(df):\n",
                "    # 1. Discount_Group 생성\n",
                "    df['Discount_Group'] = (df['Discount_offered'] > 10).astype(int)\n",
                "    \n",
                "    # 2. Weight_Category 생성\n",
                "    bins = [0, 2000, 4000, float('inf')]\n",
                "    labels = ['light', 'medium', 'heavy']\n",
                "    # 'Weight_Category' 생성 후 바로 숫자로 인코딩 (Ordinal)\n",
                "    df['Weight_Category'] = pd.cut(df['Weight_in_gms'], bins=bins, labels=labels)\n",
                "    # 모델 입력을 위해 숫자로 변환 (light:0, medium:1, heavy:2)\n",
                "    df['Weight_Category'] = df['Weight_Category'].map({'light': 0, 'medium': 1, 'heavy': 2}).astype(int)\n",
                "    \n",
                "    # 3. log_rel_price 생성\n",
                "    # 무게가 0인 경우를 대비해 아주 작은 값을 더하거나 예외처리 (이 데이터셋은 보통 양수임)\n",
                "    df['log_rel_price'] = np.log(df['Cost_of_the_Product'] / df['Weight_in_gms'])\n",
                "    \n",
                "    # 4. Product_importance: Label Encoding (순서가 있으므로 수동 매핑)\n",
                "    importance_map = {'low': 0, 'medium': 1, 'high': 2}\n",
                "    df['Product_importance'] = df['Product_importance'].map(importance_map)\n",
                "\n",
                "    # 5. prior purchase 6이상 묶기\n",
                "    df['Prior_purchases'] = df['Prior_purchases'].clip(upper=6)\n",
                "\n",
                "    #  창고별 처리 물량: 병목현상으로 인한 지연 위험\n",
                "    df['Warehouse_Load'] = df.groupby('Warehouse_block')['Warehouse_block'].transform('count')\n",
                "\n",
                "    # 상담 빈도 압박: 이전 구매 횟수 대비 상담 전화 빈도\n",
                "    df['Care_Pressure'] = df['Customer_care_calls'] / (df['Prior_purchases'] + 1)\n",
                "\n",
                "    # 고객 우선순위 지수\n",
                "    df['Priority_Score'] = df['Product_importance']/ (df['Customer_rating'] + 1)\n",
                "\n",
                "    # 예상 배송 민감도\n",
                "    df['Expectation_Gap'] = df['Cost_of_the_Product'] * df['Discount_offered']\n",
                "\n",
                "    # 배송부담지수: 운송 수단별 평균 무게 대비 해당 화물의 무게 비중\n",
                "    df['Shipping_Burden'] = df['Weight_in_gms'] / df.groupby('Mode_of_Shipment')['Weight_in_gms'].transform('mean')\n",
                "\n",
                "    # 고위험품목: 고가이면서 무거운 제품\n",
                "    # df['Is_High_Risk'] = ((df['Cost_of_the_Product'] > df['Cost_of_the_Product'].median()) & (df['Weight_in_gms'] > df['Weight_in_gms'].median())).astype(int)\n",
                "   \n",
                "    # 1. 창고별 배송수단 조합 (예: 'A_Flight', 'F_Ship')\n",
                "    # 단순 결합만으로도 특정 창고+수단 조합의 지연 패턴을 포착할 수 있습니다.\n",
                "    df['Wh_Shipment_Combo'] = df['Warehouse_block'].astype(str) + '_' + df['Mode_of_Shipment'].astype(str)\n",
                "    \n",
                "    # 모델 학습을 위해 범주형을 숫자로 매핑 (Label Encoding과 유사)\n",
                "    # AutoGluon을 쓰신다면 문자열 그대로 두어도 되지만, Scikit-learn 모델을 위해 숫자로 변환합니다.\n",
                "    from sklearn.preprocessing import LabelEncoder\n",
                "    le = LabelEncoder()\n",
                "    df['Wh_Shipment_Combo'] = le.fit_transform(df['Wh_Shipment_Combo']) + 1\n",
                "\n",
                "    return df\n",
                "\n",
                "# 각각의 데이터셋에 적용\n",
                "train_df = apply_feature_engineering(train_df)\n",
                "test_df = apply_feature_engineering(test_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "drop_columns = [\"ID\", \"Gender\", \"Discount_offered\", \"Weight_in_gms\",'Warehouse_block' ]\n",
                "train_df = train_df.drop(drop_columns, axis=1)\n",
                "test_df = test_df.drop(drop_columns, axis=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# 1. One-Hot Encoding (Warehouse_block, Mode_of_Shipment)\n",
                "# drop_first=True를 통해 다중공선성을 방지합니다.\n",
                "train_df = pd.get_dummies(train_df, columns=['Mode_of_Shipment'])\n",
                "test_df = pd.get_dummies(test_df, columns=['Mode_of_Shipment'])\n",
                "\n",
                "# 2. Scaling 대상 컬럼 선정\n",
                "# 새로 만든 log_rel_price와 기존 수치형 변수들을 포함합니다.\n",
                "scale_cols = train_df.select_dtypes(include=['number']).columns.drop('Reached.on.Time_Y.N').tolist()\n",
                "\n",
                "scaler = StandardScaler()\n",
                "\n",
                "# 중요: fit은 오직 train_df에만 수행합니다!\n",
                "train_df[scale_cols] = scaler.fit_transform(train_df[scale_cols])\n",
                "\n",
                "# test와 eval은 transform만 수행하여 데이터 누수를 차단합니다.\n",
                "test_df[scale_cols] = scaler.transform(test_df[scale_cols])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# train_df를 X_train, y_train, X_test, y_test으로 분리\n",
                "\n",
                "target = 'Reached.on.Time_Y.N'\n",
                "X = train_df.drop(label, axis=1)\n",
                "y = train_df[label]\n",
                "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Stage 1: Baseline Evaluation\n",
                "Running all models with default parameters to select the Top 5."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "baseline_models = {\n",
                "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n",
                "    'RandomForest': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
                "    'ExtraTrees': ExtraTreesClassifier(random_state=42, n_jobs=-1),\n",
                "    'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss', n_jobs=-1),\n",
                "    'LightGBM': LGBMClassifier(random_state=42, verbose=-1, n_jobs=-1),\n",
                "    'CatBoost': CatBoostClassifier(random_state=42, verbose=0, allow_writing_files=False),\n",
                "    'SVM': SVC(random_state=42, probability=True), # Default RBF\n",
                "    'KNN': KNeighborsClassifier(n_jobs=-1)\n",
                "}\n",
                "\n",
                "baseline_results = []\n",
                "\n",
                "print(\"--- Running Baseline Evaluation ---\")\n",
                "for name, model in baseline_models.items():\n",
                "    model.fit(X_train, y_train)\n",
                "    probs = model.predict_proba(X_val)[:, 1]\n",
                "    auc = roc_auc_score(y_val, probs)\n",
                "    acc = accuracy_score(y_val, model.predict(X_val))\n",
                "    \n",
                "    baseline_results.append({'Model': name, 'ROC-AUC': auc, 'Accuracy': acc})\n",
                "    print(f\"{name}: AUC={auc:.4f}, Acc={acc:.4f}\")\n",
                "\n",
                "baseline_df = pd.DataFrame(baseline_results).sort_values(by='ROC-AUC', ascending=False)\n",
                "top_5_names = baseline_df.head(5)['Model'].tolist()\n",
                "print(f\"\\nSelected Top 5 for Tuning: {top_5_names}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Stage 2: Optuna Optimization (Top 5)\n",
                "Tuning only the selected high-potential models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tuned_models = {}\n",
                "final_results = []\n",
                "\n",
                "def optimize_model(trial, model_name):\n",
                "    # Define search spaces (same as before, condensed for brevity)\n",
                "    if model_name == 'LogisticRegression':\n",
                "        params = {'C': trial.suggest_float('C', 1e-4, 100, log=True), 'solver': trial.suggest_categorical('solver', ['liblinear', 'lbfgs'])}\n",
                "        model = LogisticRegression(**params, random_state=42, max_iter=1000)\n",
                "    elif model_name == 'RandomForest':\n",
                "        params = {'n_estimators': trial.suggest_int('n_estimators', 50, 300), 'max_depth': trial.suggest_int('max_depth', 3, 20)}\n",
                "        model = RandomForestClassifier(**params, random_state=42, n_jobs=-1)\n",
                "    elif model_name == 'ExtraTrees':\n",
                "        params = {'n_estimators': trial.suggest_int('n_estimators', 50, 300), 'max_depth': trial.suggest_int('max_depth', 3, 20)}\n",
                "        model = ExtraTreesClassifier(**params, random_state=42, n_jobs=-1)\n",
                "    elif model_name == 'XGBoost':\n",
                "        params = {'n_estimators': trial.suggest_int('n_estimators', 50, 300), 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True), 'max_depth': trial.suggest_int('max_depth', 3, 10)}\n",
                "        model = XGBClassifier(**params, random_state=42, use_label_encoder=False, eval_metric='logloss', n_jobs=-1)\n",
                "    elif model_name == 'LightGBM':\n",
                "        params = {'n_estimators': trial.suggest_int('n_estimators', 50, 300), 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True), 'num_leaves': trial.suggest_int('num_leaves', 20, 100)}\n",
                "        model = LGBMClassifier(**params, random_state=42, verbose=-1, n_jobs=-1)\n",
                "    elif model_name == 'CatBoost':\n",
                "        params = {'iterations': trial.suggest_int('iterations', 50, 300), 'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True), 'depth': trial.suggest_int('depth', 3, 10)}\n",
                "        model = CatBoostClassifier(**params, random_state=42, verbose=0, allow_writing_files=False)\n",
                "    elif model_name == 'SVM':\n",
                "        params = {'C': trial.suggest_float('C', 0.1, 100, log=True), 'gamma': trial.suggest_categorical('gamma', ['scale', 'auto'])}\n",
                "        model = SVC(**params, random_state=42, probability=True)\n",
                "    elif model_name == 'KNN':\n",
                "        params = {'n_neighbors': trial.suggest_int('n_neighbors', 3, 30), 'weights': trial.suggest_categorical('weights', ['uniform', 'distance'])}\n",
                "        model = KNeighborsClassifier(**params, n_jobs=-1)\n",
                "        \n",
                "    model.fit(X_train, y_train)\n",
                "    return roc_auc_score(y_val, model.predict_proba(X_val)[:, 1])\n",
                "\n",
                "print(\"\\n--- Starting Optuna Tuning for Top 5 ---\")\n",
                "# Helper to reconstruct model from params\n",
                "def get_model_instance(name, params):\n",
                "    if name == 'LogisticRegression': return LogisticRegression(**params, random_state=42, max_iter=1000)\n",
                "    if name == 'RandomForest': return RandomForestClassifier(**params, random_state=42, n_jobs=-1)\n",
                "    if name == 'ExtraTrees': return ExtraTreesClassifier(**params, random_state=42, n_jobs=-1)\n",
                "    if name == 'XGBoost': return XGBClassifier(**params, random_state=42, use_label_encoder=False, eval_metric='logloss', n_jobs=-1)\n",
                "    if name == 'LightGBM': return LGBMClassifier(**params, random_state=42, verbose=-1, n_jobs=-1)\n",
                "    if name == 'CatBoost': return CatBoostClassifier(**params, random_state=42, verbose=0, allow_writing_files=False)\n",
                "    if name == 'SVM': return SVC(**params, random_state=42, probability=True)\n",
                "    if name == 'KNN': return KNeighborsClassifier(**params, n_jobs=-1)\n",
                "    return None\n",
                "\n",
                "for name in top_5_names:\n",
                "    study = optuna.create_study(direction='maximize')\n",
                "    study.optimize(lambda trial: optimize_model(trial, name), n_trials=2) # 2 trials per model for speed\n",
                "    \n",
                "    best_model = get_model_instance(name, study.best_params)\n",
                "    best_model.fit(X_train, y_train)\n",
                "    tuned_models[name] = best_model\n",
                "    \n",
                "    # Evaluating tuned model\n",
                "    probs = best_model.predict_proba(X_val)[:, 1]\n",
                "    metrics = {\n",
                "        'Model': f\"{name} (Tuned)\",\n",
                "        'ROC-AUC': roc_auc_score(y_val, probs),\n",
                "        'Accuracy': accuracy_score(y_val, best_model.predict(X_val)),\n",
                "        'F1-Score': f1_score(y_val, best_model.predict(X_val))\n",
                "    }\n",
                "    final_results.append(metrics)\n",
                "    print(f\"{name} Tuned: AUC={metrics['ROC-AUC']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Ensemble Models (Using Tuned Top 5)\n",
                "Combining the optimized models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "estimators = [(name, model) for name, model in tuned_models.items()]\n",
                "\n",
                "# Voting\n",
                "vote = VotingClassifier(estimators=estimators, voting='soft', n_jobs=-1)\n",
                "vote.fit(X_train, y_train)\n",
                "final_results.append({\n",
                "    'Model': 'Vote_Ensemble',\n",
                "    'ROC-AUC': roc_auc_score(y_val, vote.predict_proba(X_val)[:, 1]),\n",
                "    'Accuracy': accuracy_score(y_val, vote.predict(X_val)),\n",
                "    'F1-Score': f1_score(y_val, vote.predict(X_val))\n",
                "})\n",
                "\n",
                "# Stacking\n",
                "stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=5, n_jobs=-1)\n",
                "stack.fit(X_train, y_train)\n",
                "final_results.append({\n",
                "    'Model': 'Stack_Ensemble',\n",
                "    'ROC-AUC': roc_auc_score(y_val, stack.predict_proba(X_val)[:, 1]),\n",
                "    'Accuracy': accuracy_score(y_val, stack.predict(X_val)),\n",
                "    'F1-Score': f1_score(y_val, stack.predict(X_val))\n",
                "})\n",
                "\n",
                "tuned_models['Vote_Ensemble'] = vote\n",
                "tuned_models['Stack_Ensemble'] = stack\n",
                "\n",
                "results_df = pd.DataFrame(final_results).sort_values(by='ROC-AUC', ascending=False)\n",
                "print(\"\\n--- Final Model Leaderboard ---\")\n",
                "print(results_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Final Predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "final_preds = pd.DataFrame({'ID': test_ids}) if 'test_ids' in locals() else pd.DataFrame()\n",
                "\n",
                "for name in results_df['Model'].head(5):\n",
                "    # Strip '(Tuned)' if present to match key\n",
                "    key = name.replace(' (Tuned)', '')\n",
                "    model = tuned_models[key]\n",
                "    final_preds[f'{name}_Prob'] = model.predict_proba(X_test_final)[:, 1]\n",
                "    final_preds[f'{name}_Pred'] = model.predict(X_test_final)\n",
                "\n",
                "final_preds.to_csv('data/final_predictions.csv', index=False)\n",
                "print(\"Predictions saved to data/final_predictions.csv\")\n",
                "results_df.to_csv('data/final_performance.csv', index=False)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
